# deep-learning-distilled
Notes on some important deep learning topics and paper summaries.</br>
Feel free to contribute.</br>
Documents have corresponding latex file you can edit.

### Notes added so far
* **[Information, Entropy, Cross-Entropy: ML perspective:](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/notes/Information%2C%20Entropy%2C%20Cross-Entropy%20ML%20perspective.pdf)** Basic of information theory, why *log* is used to represent information. entropy, cross entropy, KL divergence, likelihood, why cross entropy loss is used in machine learning. 
* **[Gradient Descent Optimizations:](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/notes/Gradient%20Descent%20Optimizations.pdf)** Three gradient descent varients, chalenges with vanilla gradient descent, momentum, Nesterov accelerated gradient, Adagrad, RMSprop, Adadelta, Adam.
* **[Common activation functions used in neural net:](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/notes/Common%20activation%20functions%20used%20in%20neural%20net.pdf)** why we need to use activation function, desirable properties of activation, sigmoid, tanh, relu, prelu, elu.
* **[Why ReLU(instead of sigmoid/tanh)?:](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/notes/Why%20ReLU.pdf)** why ReLU is better suited for deep learning compared to sigmoid or tanh, some of the potential problems with ReLU and how to mitigate them.

### Paper Summary
1. [How transferable are features in deep neural networks?](https://arxiv.org/abs/1411.1792)
2. [Learning and transferring mid-Level image representations using convolutional neural networks](http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf)
3. [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)
4. [Sequence to Sequence Learning with Neural Networks](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.pdf)
5. [Distributed Representations of Sentences and Documents](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/Distributed%20Representations%20of%20Sentences%20and%20Documents.pdf)
6. [VGG](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/Very%20Deep%20Convolutional%20Networks%20for%20Large-Scale%20Image%20Recognition(VGG).pdf)
7. [ResNet](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/Deep%20Residual%20Learning%20for%20Image%20Recognition.pdf)
8. [Deep Sparse Rectifier Neural Networks](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/Deep%20Sparse%20Rectifier%20Neural%20Networks.pdf)
9. [Network in Network](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/Network%20in%20Network.pdf)
10. [GoogLeNet](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/Going%20deeper%20with%20convolutions.pdf)
11. [MobileNets](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/MobileNets-Efficient%20Convolutional%20Neural%20Networks%20for%20Mobile%20Vision%20Applications.pdf)
12. [AlexNet](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/ImageNet%20Classification%20with%20Deep%20Convolutional%20Neural%20Networks.pdf)
13. [Inception-V2](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/Rethinking%20the%20Inception%20Architecture%20for%20Computer%20Vision.pdf)
14. [Inception-V4](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/Inception-v4%2C%20Inception-ResNet%20and%20the%20Impact%20of%20Residual%20Connections%20on%20Learning.pdf)
15. [Dropout](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/Dropout-%20A%20Simple%20Way%20to%20Prevent%20Neural%20Networks%20from%20Overfitting.pdf)
16. [Efficient Estimation of Word Representations in Vector Space](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/Efficient%20Estimation%20of%20Word%20Representations%20in%20Vector%20Space.pdf)
17. [A Convolutional Neural Network for Modelling Sentences](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/A%20Convolutional%20Neural%20Network%20for%20Modelling%20Sentences.pdf)
18. [Effective Approaches to Attention-based Neural Machine Translation](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/Effective%20Approaches%20to%20Attention-based%20Neural%20Machine%20Translation.pdf)
19. [Neural Machine Translation by Jointly Learning to Align and Translate](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.pdf)
20. [Large-scale Video Classification with Convolutional Neural Networks](https://github.com/xashru/deep-learning-distilled/blob/master/pdf/paper_summary/Large-scale%20Video%20Classification%20with%20Convolutional%20Neural%20Networks.pdf)

### Notes to add
- [ ] Regularization
- [ ] Different types of Losses
- [ ] Activation functions: swish
